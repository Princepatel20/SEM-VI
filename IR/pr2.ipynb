// Write a python program to perform tokenization And stemming for any given document file.
// Tokenization Program :

from google.colab import drive
drive.mount('/content/drive')
import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
def tokenize_document(document_path):
with open(document_path, 'r', encoding='utf-8') as file:
document = file.read()
tokens = word_tokenize(document)
stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if word.lower() not in stop_words]
return filtered_tokens
if __name__ == "__main__":
document_path = '/content/drive/My Drive/prince.txt'
tokenized_result = tokenize_document(document_path)
print("Tokenized Result:")
print(tokenized_result)

// Stemming Program :

import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
def stem(document_path):
with open(document_path, 'r', encoding='utf-8') as f1:
document = f1.read()
tokens = word_tokenize(document)
stemmer = PorterStemmer()
tokens = [stemmer.stem(word) for word in tokens]
return tokens
if __name__ == "__main__":
document_path = '/content/drive/My Drive/prince.txt'
result = stem(document_path)
print("Stemmed Result:")
print(result)
